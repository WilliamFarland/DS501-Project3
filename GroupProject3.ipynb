{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project 3\n",
    "# DS 501 - Introduction to Data Science\n",
    "# Group 3&5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "\n",
    "# Problem 1 (20 points): Complete Exercise 2: Sentiment Analysis on Movie Reviews from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1; Problem 1: Downloading Data\n",
    "Modify the solution on Exercise 2 so that it can run in this iPython notebook\n",
    "* This will likely involved moving around data files and/or small modifications to the script."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download Data Script"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz (3 MB)\n",
      "Checking the integrity of the archive\n",
      "Decompressing review_polarity.tar.gz\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Script to download the movie review dataset\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from hashlib import sha256\n",
    "import tarfile\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "URL = \"http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\"\n",
    "\n",
    "ARCHIVE_SHA256 = \"fc0dccc2671af5db3c5d8f81f77a1ebfec953ecdd422334062df61ede36b2179\"\n",
    "ARCHIVE_NAME = Path(URL.rsplit(\"/\", 1)[1])\n",
    "DATA_FOLDER = Path(\"txt_sentoken\")\n",
    "\n",
    "\n",
    "if not DATA_FOLDER.exists():\n",
    "\n",
    "    if not ARCHIVE_NAME.exists():\n",
    "        print(\"Downloading dataset from %s (3 MB)\" % URL)\n",
    "        opener = urlopen(URL)\n",
    "        with open(ARCHIVE_NAME, \"wb\") as archive:\n",
    "            archive.write(opener.read())\n",
    "\n",
    "    try:\n",
    "        print(\"Checking the integrity of the archive\")\n",
    "        assert sha256(ARCHIVE_NAME.read_bytes()).hexdigest() == ARCHIVE_SHA256\n",
    "\n",
    "        print(\"Decompressing %s\" % ARCHIVE_NAME)\n",
    "        with tarfile.open(ARCHIVE_NAME, \"r:gz\") as archive:\n",
    "            archive.extractall(path=\".\")\n",
    "\n",
    "    finally:\n",
    "        ARCHIVE_NAME.unlink()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2; Problem 1: Sentiment Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='honeydew';this.parentNode.removeChild(this)\" style=\"display:none\">"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check number of samples and if prev script(s) ran properly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# the training data folder must be passed as first argument\n",
    "movie_reviews_data_folder = DATA_FOLDER\n",
    "dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
    "print(\"n_samples: %d\" % len(dataset.data))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sentiment Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 params - {'vect__ngram_range': (1, 1)}; mean - 0.84; std - 0.01\n",
      "1 params - {'vect__ngram_range': (1, 2)}; mean - 0.85; std - 0.01\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.85      0.90      0.87       244\n",
      "         pos       0.90      0.85      0.87       256\n",
      "\n",
      "    accuracy                           0.87       500\n",
      "   macro avg       0.87      0.87      0.87       500\n",
      "weighted avg       0.87      0.87      0.87       500\n",
      "\n",
      "[[219  25]\n",
      " [ 39 217]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split the dataset in training and test set:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
    "\n",
    "# TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
    "# that are too rare or too frequent\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
    "    ('clf', LinearSVC(C=5000)),\n",
    "])\n",
    "\n",
    "# TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "# more useful.\n",
    "# Fit the pipeline on the training set using grid search for the parameters\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "}\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "grid_search.fit(docs_train, y_train)\n",
    "\n",
    "# TASK: print the mean and std for each candidate along with the parameter\n",
    "# settings for all the candidates explored by grid search.\n",
    "n_candidates = len(grid_search.cv_results_['params'])\n",
    "for i in range(n_candidates):\n",
    "    print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
    "          % (grid_search.cv_results_['params'][i],\n",
    "             grid_search.cv_results_['mean_test_score'][i],\n",
    "             grid_search.cv_results_['std_test_score'][i]))\n",
    "\n",
    "# TASK: Predict the outcome on the testing set and store it in a variable\n",
    "# named y_predicted\n",
    "y_predicted = grid_search.predict(docs_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                    target_names=dataset.target_names))\n",
    "\n",
    "# Print and plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.matshow(cm)\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Problem 2 (20 points): Explore the Scikit-learn TfidfVectorizer Class\n",
    "\n",
    "**Read the documentation for the TfidfVectorizer class at https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 1; Problem 2:\n",
    " Define the term frequency–inverse document frequency (TF-IDF) statistic (https://en.wikipedia.org/wiki/Tf%E2%80%93idf) will likely help."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of TF-IDF\n",
    "\n",
    "TF-IDF stands for \"Term Frequency-Inverse Document Frequency.\" It is a way of figuring out how important a word is in a document or a piece of writing.\n",
    "\n",
    "Let's say you have a book about cats. In this book, the word \"cat\" is used a lot because it's the main topic of the book. But the word \"dog\" is only used a few times because it's not really related to the subject of the book.\n",
    "\n",
    "TF-IDF takes into account both the number of times a word appears in a document (the \"Term Frequency\") and how rare that word is in all the other documents (the \"Inverse Document Frequency\").\n",
    "\n",
    "So, in the example of the book about cats, the word \"cat\" would have a high TF-IDF score because it appears frequently in the book and is relevant to the topic. The word \"dog\" would have a low TF-IDF score because it appears infrequently and is not as relevant to the topic.\n",
    "\n",
    "Basically, TF-IDF helps us understand which words are most important in a document and which ones are less important."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2; Problem 2:\n",
    " Run the TfidfVectorizer class on the training data above (docs_train).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: (1500, 253)\n"
     ]
    },
    {
     "data": {
      "text/plain": "      about    acting  action     actor    actors  actually     after  \\\n0  0.112499  0.000000     0.0  0.000000  0.000000  0.000000  0.000000   \n1  0.104921  0.000000     0.0  0.000000  0.075476  0.035823  0.000000   \n2  0.000000  0.084449     0.0  0.092692  0.085006  0.000000  0.058941   \n3  0.026597  0.000000     0.0  0.000000  0.000000  0.000000  0.099495   \n4  0.120303  0.128961     0.0  0.094366  0.000000  0.000000  0.000000   \n\n      again       all    almost  ...   without      work     world     would  \\\n0  0.000000  0.000000  0.062856  ...  0.000000  0.114916  0.188757  0.000000   \n1  0.037328  0.116906  0.070346  ...  0.109615  0.160763  0.000000  0.025171   \n2  0.000000  0.043889  0.000000  ...  0.000000  0.000000  0.079309  0.113398   \n3  0.000000  0.024696  0.000000  ...  0.000000  0.000000  0.000000  0.063807   \n4  0.000000  0.067023  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n\n       year     years       yet       you     young      your  \n0  0.061558  0.061862  0.000000  0.074585  0.000000  0.064414  \n1  0.068894  0.000000  0.000000  0.062604  0.037863  0.000000  \n2  0.155187  0.000000  0.175855  0.000000  0.000000  0.000000  \n3  0.000000  0.000000  0.000000  0.079349  0.000000  0.000000  \n4  0.118492  0.000000  0.000000  0.023928  0.000000  0.000000  \n\n[5 rows x 253 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>about</th>\n      <th>acting</th>\n      <th>action</th>\n      <th>actor</th>\n      <th>actors</th>\n      <th>actually</th>\n      <th>after</th>\n      <th>again</th>\n      <th>all</th>\n      <th>almost</th>\n      <th>...</th>\n      <th>without</th>\n      <th>work</th>\n      <th>world</th>\n      <th>would</th>\n      <th>year</th>\n      <th>years</th>\n      <th>yet</th>\n      <th>you</th>\n      <th>young</th>\n      <th>your</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.112499</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.062856</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.114916</td>\n      <td>0.188757</td>\n      <td>0.000000</td>\n      <td>0.061558</td>\n      <td>0.061862</td>\n      <td>0.000000</td>\n      <td>0.074585</td>\n      <td>0.000000</td>\n      <td>0.064414</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.104921</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.075476</td>\n      <td>0.035823</td>\n      <td>0.000000</td>\n      <td>0.037328</td>\n      <td>0.116906</td>\n      <td>0.070346</td>\n      <td>...</td>\n      <td>0.109615</td>\n      <td>0.160763</td>\n      <td>0.000000</td>\n      <td>0.025171</td>\n      <td>0.068894</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.062604</td>\n      <td>0.037863</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.084449</td>\n      <td>0.0</td>\n      <td>0.092692</td>\n      <td>0.085006</td>\n      <td>0.000000</td>\n      <td>0.058941</td>\n      <td>0.000000</td>\n      <td>0.043889</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.079309</td>\n      <td>0.113398</td>\n      <td>0.155187</td>\n      <td>0.000000</td>\n      <td>0.175855</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.026597</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.099495</td>\n      <td>0.000000</td>\n      <td>0.024696</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.063807</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.079349</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.120303</td>\n      <td>0.128961</td>\n      <td>0.0</td>\n      <td>0.094366</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.067023</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.118492</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.023928</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 253 columns</p>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=0.2, max_df=0.95)\n",
    "\n",
    "\n",
    "vectors = vectorizer.fit_transform(docs_train)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "\n",
    "print(f\"Output Shape: {df.shape}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 3; Problem 2:\n",
    "- Explore the min_df and max_df parameters of TfidfVectorizer.\n",
    "- What do they mean? How do they change the features you get?\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Book Definition\n",
    "\n",
    "**min_df definition: min_dffloat or int, default=1**\n",
    "    - When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float in range of [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "**max_df definition: max_dffloat or int, default=1.0**\n",
    "    - When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float in range [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "Explanation:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 4; Problem 2\n",
    "Explore the ngram_range parameter of TfidfVectorizer. What does it mean? How does it change the features you get? (Note, large values  of ngram_range may take a long time to run!)\n",
    "\n",
    "**ngram_rangetuple (min_n, max_n), default=(1, 1)**\n",
    "    - The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only applies if analyzer is not callable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Answer:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem 3 (20 points): Machine Learning Algorithms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Based upon Problem 2, pick some parameters for TfidfVectorizer\n",
    "    * \"fit\" your TfidfVectorizer using docs_train\n",
    "    * Compute \"Xtrain\", a Tf-idf-weighted document-term matrix using the transform function on docs_train\n",
    "    * Compute \"Xtest\", a Tf-idf-weighted document-term matrix using the transform function on docs_test\n",
    "    * Note, be sure to use the same Tf-idf-weighted class (**\"fit\" using docs_train**) to transform **both** docs_test and docs_train\n",
    "* Examine two classifiers provided by scikit-learn \n",
    "    * LinearSVC\n",
    "    * KNeighborsClassifier\n",
    "    * Try a number of different parameter settings for each and judge your performance using a confusion matrix (see Problem 1 for an example).\n",
    "* Does one classifier, or one set of parameters work better?\n",
    "    * Why do you think it might be working better?\n",
    "* For a particular choice of parameters and classifier, look at 2 examples where the prediction was incorrect.\n",
    "    * Can you conjecture on why the classifier made a mistake for this prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "# Your code goes here\n",
    "# Add as many cells as you need\n",
    "#------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 (20 points): Open Ended Question:  Finding the Right Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can you find a two-dimensional plot in which the positive and negative reviews are separated?\n",
    "    * This problem is hard since you will likely have thousands of features for review, and you will need to transform these thousands of features into just two numbers (so that you can make a 2D plot).\n",
    "* Note, I was not able to find such a plot myself!\n",
    "    * So, this problem is about **trying** but perhaps **not necessarily succeeding**!\n",
    "* I tried two things, neither of which worked very well.\n",
    "    * I first plotted the length of the review versus the number of features we compute that are in that review\n",
    "    * Second I used Principle Component Analysis on a subset of the features.\n",
    "* Can you do better than I did!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "# Your code goes here\n",
    "# Add as many cells as you need\n",
    "#------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "0.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
