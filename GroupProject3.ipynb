{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project 3\n",
    "# DS 501 - Introduction to Data Science\n",
    "# Group 3&5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "\n",
    "# Problem 1 (20 points): Complete Exercise 2: Sentiment Analysis on Movie Reviews from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1; Problem 1: Downloading Data\n",
    "Modify the solution on Exercise 2 so that it can run in this iPython notebook\n",
    "* This will likely involved moving around data files and/or small modifications to the script."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download Data Script"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\"\"\"Script to download the movie review dataset\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from hashlib import sha256\n",
    "import tarfile\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "URL = \"http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\"\n",
    "\n",
    "ARCHIVE_SHA256 = \"fc0dccc2671af5db3c5d8f81f77a1ebfec953ecdd422334062df61ede36b2179\"\n",
    "ARCHIVE_NAME = Path(URL.rsplit(\"/\", 1)[1])\n",
    "DATA_FOLDER = Path(\"txt_sentoken\")\n",
    "\n",
    "\n",
    "if not DATA_FOLDER.exists():\n",
    "\n",
    "    if not ARCHIVE_NAME.exists():\n",
    "        print(\"Downloading dataset from %s (3 MB)\" % URL)\n",
    "        opener = urlopen(URL)\n",
    "        with open(ARCHIVE_NAME, \"wb\") as archive:\n",
    "            archive.write(opener.read())\n",
    "\n",
    "    try:\n",
    "        print(\"Checking the integrity of the archive\")\n",
    "        assert sha256(ARCHIVE_NAME.read_bytes()).hexdigest() == ARCHIVE_SHA256\n",
    "\n",
    "        print(\"Decompressing %s\" % ARCHIVE_NAME)\n",
    "        with tarfile.open(ARCHIVE_NAME, \"r:gz\") as archive:\n",
    "            archive.extractall(path=\".\")\n",
    "\n",
    "    finally:\n",
    "        ARCHIVE_NAME.unlink()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2; Problem 1: Sentiment Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check number of samples and if prev script(s) ran properly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# the training data folder must be passed as first argument\n",
    "movie_reviews_data_folder = DATA_FOLDER\n",
    "dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
    "print(\"n_samples: %d\" % len(dataset.data))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sentiment Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 params - {'vect__ngram_range': (1, 1)}; mean - 0.86; std - 0.02\n",
      "1 params - {'vect__ngram_range': (1, 2)}; mean - 0.87; std - 0.02\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.86      0.85      0.85       239\n",
      "         pos       0.86      0.87      0.87       261\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.86      0.86      0.86       500\n",
      "weighted avg       0.86      0.86      0.86       500\n",
      "\n",
      "[[203  36]\n",
      " [ 33 228]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split the dataset in training and test set:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
    "\n",
    "# TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
    "# that are too rare or too frequent\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
    "    ('clf', LinearSVC(C=5000)),\n",
    "])\n",
    "\n",
    "# TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "# more useful.\n",
    "# Fit the pipeline on the training set using grid search for the parameters\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "}\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "grid_search.fit(docs_train, y_train)\n",
    "\n",
    "# TASK: print the mean and std for each candidate along with the parameter\n",
    "# settings for all the candidates explored by grid search.\n",
    "n_candidates = len(grid_search.cv_results_['params'])\n",
    "for i in range(n_candidates):\n",
    "    print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
    "          % (grid_search.cv_results_['params'][i],\n",
    "             grid_search.cv_results_['mean_test_score'][i],\n",
    "             grid_search.cv_results_['std_test_score'][i]))\n",
    "\n",
    "# TASK: Predict the outcome on the testing set and store it in a variable\n",
    "# named y_predicted\n",
    "y_predicted = grid_search.predict(docs_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                    target_names=dataset.target_names))\n",
    "\n",
    "# Print and plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.matshow(cm)\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Problem 2 (20 points): Explore the Scikit-learn TfidfVectorizer Class\n",
    "\n",
    "**Read the documentation for the TfidfVectorizer class at https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 1; Problem 2:\n",
    " Define the term frequencyâ€“inverse document frequency (TF-IDF) statistic (https://en.wikipedia.org/wiki/Tf%E2%80%93idf) will likely help."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of TF-IDF\n",
    "\n",
    "TF-IDF stands for \"Term Frequency-Inverse Document Frequency.\" It is a way of figuring out how important a word is in a document or a piece of writing.\n",
    "\n",
    "Let's say you have a book about cats. In this book, the word \"cat\" is used a lot because it's the main topic of the book. But the word \"dog\" is only used a few times because it's not really related to the subject of the book.\n",
    "\n",
    "TF-IDF takes into account both the number of times a word appears in a document (the \"Term Frequency\") and how rare that word is in all the other documents (the \"Inverse Document Frequency\").\n",
    "\n",
    "So, in the example of the book about cats, the word \"cat\" would have a high TF-IDF score because it appears frequently in the book and is relevant to the topic. The word \"dog\" would have a low TF-IDF score because it appears infrequently and is not as relevant to the topic.\n",
    "\n",
    "Basically, TF-IDF helps us understand which words are most important in a document and which ones are less important."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2; Problem 2:\n",
    " Run the TfidfVectorizer class on the training data above (docs_train).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: (1500, 251)\n"
     ]
    },
    {
     "data": {
      "text/plain": "      about    acting    action  actor    actors  actually     after  \\\n0  0.151729  0.000000  0.000000    0.0  0.053878       0.0  0.037662   \n1  0.105541  0.000000  0.000000    0.0  0.000000       0.0  0.000000   \n2  0.039701  0.069282  0.000000    0.0  0.000000       0.0  0.000000   \n3  0.000000  0.000000  0.000000    0.0  0.000000       0.0  0.000000   \n4  0.042407  0.222015  0.143885    0.0  0.150585       0.0  0.000000   \n\n      again       all  almost  ...  without      work     world  would  \\\n0  0.054293  0.084233     0.0  ...      0.0  0.000000  0.050958    0.0   \n1  0.000000  0.000000     0.0  ...      0.0  0.000000  0.000000    0.0   \n2  0.000000  0.000000     0.0  ...      0.0  0.000000  0.000000    0.0   \n3  0.000000  0.000000     0.0  ...      0.0  0.000000  0.000000    0.0   \n4  0.000000  0.117712     0.0  ...      0.0  0.065406  0.071211    0.0   \n\n      year     years       yet       you  young      your  \n0  0.05065  0.100791  0.000000  0.121046    0.0  0.104689  \n1  0.00000  0.000000  0.000000  0.000000    0.0  0.000000  \n2  0.00000  0.000000  0.073903  0.237542    0.0  0.000000  \n3  0.00000  0.000000  0.000000  0.000000    0.0  0.000000  \n4  0.07078  0.000000  0.000000  0.042289    0.0  0.000000  \n\n[5 rows x 251 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>about</th>\n      <th>acting</th>\n      <th>action</th>\n      <th>actor</th>\n      <th>actors</th>\n      <th>actually</th>\n      <th>after</th>\n      <th>again</th>\n      <th>all</th>\n      <th>almost</th>\n      <th>...</th>\n      <th>without</th>\n      <th>work</th>\n      <th>world</th>\n      <th>would</th>\n      <th>year</th>\n      <th>years</th>\n      <th>yet</th>\n      <th>you</th>\n      <th>young</th>\n      <th>your</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.151729</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.053878</td>\n      <td>0.0</td>\n      <td>0.037662</td>\n      <td>0.054293</td>\n      <td>0.084233</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.050958</td>\n      <td>0.0</td>\n      <td>0.05065</td>\n      <td>0.100791</td>\n      <td>0.000000</td>\n      <td>0.121046</td>\n      <td>0.0</td>\n      <td>0.104689</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.105541</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.039701</td>\n      <td>0.069282</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.073903</td>\n      <td>0.237542</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.042407</td>\n      <td>0.222015</td>\n      <td>0.143885</td>\n      <td>0.0</td>\n      <td>0.150585</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.117712</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.065406</td>\n      <td>0.071211</td>\n      <td>0.0</td>\n      <td>0.07078</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.042289</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 251 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=0.2, max_df=0.95)\n",
    "\n",
    "\n",
    "vectors = vectorizer.fit_transform(docs_train)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "\n",
    "print(f\"Output Shape: {df.shape}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 3; Problem 2:\n",
    "- Explore the min_df and max_df parameters of TfidfVectorizer.\n",
    "- What do they mean? How do they change the features you get?\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Book Definition\n",
    "\n",
    "**min_df definition: min_dffloat or int, default=1**\n",
    "    - When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float in range of [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "**max_df definition: max_dffloat or int, default=1.0**\n",
    "    - When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float in range [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "Explanation:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 4; Problem 2\n",
    "Explore the ngram_range parameter of TfidfVectorizer. What does it mean? How does it change the features you get? (Note, large values  of ngram_range may take a long time to run!)\n",
    "\n",
    "**ngram_rangetuple (min_n, max_n), default=(1, 1)**\n",
    "    - The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only applies if analyzer is not callable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Answer:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem 3 (20 points): Machine Learning Algorithms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Based upon Problem 2, pick some parameters for TfidfVectorizer\n",
    "    * \"fit\" your TfidfVectorizer using docs_train\n",
    "    * Compute \"Xtrain\", a Tf-idf-weighted document-term matrix using the transform function on docs_train\n",
    "    * Compute \"Xtest\", a Tf-idf-weighted document-term matrix using the transform function on docs_test\n",
    "    * Note, be sure to use the same Tf-idf-weighted class (**\"fit\" using docs_train**) to transform **both** docs_test and docs_train\n",
    "* Examine two classifiers provided by scikit-learn \n",
    "    * LinearSVC\n",
    "    * KNeighborsClassifier\n",
    "    * Try a number of different parameter settings for each and judge your performance using a confusion matrix (see Problem 1 for an example).\n",
    "* Does one classifier, or one set of parameters work better?\n",
    "    * Why do you think it might be working better?\n",
    "* For a particular choice of parameters and classifier, look at 2 examples where the prediction was incorrect.\n",
    "    * Can you conjecture on why the classifier made a mistake for this prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "GridSearchCV(estimator=Pipeline(steps=[('vect',\n                                        TfidfVectorizer(max_df=0.9, min_df=3)),\n                                       ('clf',\n                                        KNeighborsClassifier(n_neighbors=3))]),\n             n_jobs=-1,\n             param_grid={'clf__n_neighbors': [3, 12, 15, 20],\n                         'vect__max_df': [0.7, 0.8, 0.9, 0.95],\n                         'vect__min_df': [3, 4, 5, 6],\n                         'vect__ngram_range': [(1, 1), (1, 3)]})"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=3, max_df=0.90)\n",
    "\n",
    "pipeline1 = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('clf', LinearSVC(C=15000)),\n",
    "])\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('clf', KNeighborsClassifier(n_neighbors=3)),\n",
    "])\n",
    "\n",
    "\n",
    "parameters1 = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 3)],\n",
    "    'vect__min_df': [3, 4, 5, 6],\n",
    "    'vect__max_df': [0.7, 0.8, 0.9, 0.95]\n",
    "}\n",
    "\n",
    "parameters2 = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 3)],\n",
    "    'clf__n_neighbors': [3, 12, 15, 20],\n",
    "    'vect__min_df': [3, 4, 5, 6],\n",
    "    'vect__max_df': [0.7, 0.8, 0.9, 0.95]\n",
    "}\n",
    "\n",
    "grid_search_linear = GridSearchCV(pipeline1, parameters1, n_jobs=-1)\n",
    "grid_search_linear.fit(docs_train, y_train)\n",
    "\n",
    "grid_search_knn = GridSearchCV(pipeline2,parameters2, n_jobs=-1)\n",
    "grid_search_knn.fit(docs_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Linear: \n",
      "\n",
      "0 params - {'vect__max_df': 0.7, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.85; std - 0.03\n",
      "1 params - {'vect__max_df': 0.7, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.87; std - 0.02\n",
      "2 params - {'vect__max_df': 0.7, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.85; std - 0.03\n",
      "3 params - {'vect__max_df': 0.7, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.87; std - 0.02\n",
      "4 params - {'vect__max_df': 0.7, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.85; std - 0.03\n",
      "5 params - {'vect__max_df': 0.7, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.87; std - 0.02\n",
      "6 params - {'vect__max_df': 0.7, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.84; std - 0.03\n",
      "7 params - {'vect__max_df': 0.7, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.87; std - 0.02\n",
      "8 params - {'vect__max_df': 0.8, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.85; std - 0.02\n",
      "9 params - {'vect__max_df': 0.8, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.87; std - 0.02\n",
      "10 params - {'vect__max_df': 0.8, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.85; std - 0.02\n",
      "11 params - {'vect__max_df': 0.8, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.87; std - 0.02\n",
      "12 params - {'vect__max_df': 0.8, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.85; std - 0.03\n",
      "13 params - {'vect__max_df': 0.8, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.87; std - 0.02\n",
      "14 params - {'vect__max_df': 0.8, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.84; std - 0.03\n",
      "15 params - {'vect__max_df': 0.8, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.87; std - 0.02\n",
      "16 params - {'vect__max_df': 0.9, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.86; std - 0.02\n",
      "17 params - {'vect__max_df': 0.9, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.87; std - 0.01\n",
      "18 params - {'vect__max_df': 0.9, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.86; std - 0.02\n",
      "19 params - {'vect__max_df': 0.9, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.87; std - 0.02\n",
      "20 params - {'vect__max_df': 0.9, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.85; std - 0.02\n",
      "21 params - {'vect__max_df': 0.9, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.87; std - 0.02\n",
      "22 params - {'vect__max_df': 0.9, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.85; std - 0.02\n",
      "23 params - {'vect__max_df': 0.9, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.87; std - 0.02\n",
      "24 params - {'vect__max_df': 0.95, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.86; std - 0.02\n",
      "25 params - {'vect__max_df': 0.95, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.87; std - 0.02\n",
      "26 params - {'vect__max_df': 0.95, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.85; std - 0.02\n",
      "27 params - {'vect__max_df': 0.95, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.87; std - 0.02\n",
      "28 params - {'vect__max_df': 0.95, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.85; std - 0.02\n",
      "29 params - {'vect__max_df': 0.95, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.87; std - 0.02\n",
      "30 params - {'vect__max_df': 0.95, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.85; std - 0.02\n",
      "31 params - {'vect__max_df': 0.95, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.87; std - 0.02\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.87      0.87       239\n",
      "         pos       0.88      0.88      0.88       261\n",
      "\n",
      "    accuracy                           0.87       500\n",
      "   macro avg       0.87      0.87      0.87       500\n",
      "weighted avg       0.87      0.87      0.87       500\n",
      "\n",
      "[[207  32]\n",
      " [ 32 229]]\n",
      "Performance of KNN: \n",
      "\n",
      "0 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.7, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.64; std - 0.02\n",
      "1 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.7, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.65; std - 0.02\n",
      "2 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.7, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.65; std - 0.01\n",
      "3 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.7, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.64; std - 0.02\n",
      "4 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.7, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.66; std - 0.02\n",
      "5 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.7, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.65; std - 0.02\n",
      "6 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.7, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.65; std - 0.02\n",
      "7 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.7, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.66; std - 0.02\n",
      "8 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.8, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.65; std - 0.01\n",
      "9 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.8, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.64; std - 0.02\n",
      "10 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.8, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.66; std - 0.01\n",
      "11 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.8, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.65; std - 0.02\n",
      "12 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.8, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.66; std - 0.01\n",
      "13 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.8, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.66; std - 0.02\n",
      "14 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.8, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.66; std - 0.01\n",
      "15 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.8, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.67; std - 0.02\n",
      "16 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.9, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.65; std - 0.02\n",
      "17 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.9, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.64; std - 0.01\n",
      "18 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.9, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.66; std - 0.01\n",
      "19 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.9, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.65; std - 0.01\n",
      "20 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.9, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.66; std - 0.01\n",
      "21 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.9, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.66; std - 0.01\n",
      "22 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.9, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.66; std - 0.01\n",
      "23 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.9, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.67; std - 0.02\n",
      "24 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.95, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.65; std - 0.02\n",
      "25 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.95, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.65; std - 0.02\n",
      "26 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.95, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.66; std - 0.01\n",
      "27 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.95, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.65; std - 0.02\n",
      "28 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.95, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.66; std - 0.01\n",
      "29 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.95, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.66; std - 0.01\n",
      "30 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.95, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.66; std - 0.01\n",
      "31 params - {'clf__n_neighbors': 3, 'vect__max_df': 0.95, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.67; std - 0.02\n",
      "32 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.7, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.66; std - 0.01\n",
      "33 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.7, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.69; std - 0.03\n",
      "34 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.7, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.68; std - 0.02\n",
      "35 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.7, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.70; std - 0.02\n",
      "36 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.7, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.68; std - 0.02\n",
      "37 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.7, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.69; std - 0.02\n",
      "38 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.7, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.69; std - 0.02\n",
      "39 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.7, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.69; std - 0.03\n",
      "40 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.8, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.67; std - 0.03\n",
      "41 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.8, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.70; std - 0.02\n",
      "42 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.8, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.68; std - 0.03\n",
      "43 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.8, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.70; std - 0.03\n",
      "44 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.8, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.68; std - 0.03\n",
      "45 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.8, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.69; std - 0.04\n",
      "46 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.8, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.67; std - 0.03\n",
      "47 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.8, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.69; std - 0.03\n",
      "48 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.9, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.68; std - 0.02\n",
      "49 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.9, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.69; std - 0.02\n",
      "50 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.9, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.69; std - 0.02\n",
      "51 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.9, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.69; std - 0.03\n",
      "52 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.9, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.69; std - 0.02\n",
      "53 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.9, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.70; std - 0.03\n",
      "54 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.9, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.69; std - 0.02\n",
      "55 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.9, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.69; std - 0.03\n",
      "56 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.95, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.69; std - 0.02\n",
      "57 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.95, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.69; std - 0.03\n",
      "58 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.95, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.69; std - 0.03\n",
      "59 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.95, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.69; std - 0.03\n",
      "60 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.95, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.69; std - 0.03\n",
      "61 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.95, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.70; std - 0.03\n",
      "62 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.95, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.69; std - 0.02\n",
      "63 params - {'clf__n_neighbors': 12, 'vect__max_df': 0.95, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.69; std - 0.03\n",
      "64 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.7, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.68; std - 0.03\n",
      "65 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.7, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.70; std - 0.03\n",
      "66 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.7, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.68; std - 0.02\n",
      "67 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.7, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.70; std - 0.02\n",
      "68 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.7, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.69; std - 0.03\n",
      "69 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.7, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.70; std - 0.03\n",
      "70 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.7, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.68; std - 0.02\n",
      "71 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.7, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.70; std - 0.02\n",
      "72 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.8, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.68; std - 0.03\n",
      "73 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.8, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.70; std - 0.03\n",
      "74 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.8, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.69; std - 0.02\n",
      "75 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.8, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.70; std - 0.03\n",
      "76 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.8, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.68; std - 0.03\n",
      "77 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.8, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.70; std - 0.03\n",
      "78 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.8, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.69; std - 0.03\n",
      "79 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.8, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.70; std - 0.03\n",
      "80 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.9, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.69; std - 0.03\n",
      "81 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.9, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.69; std - 0.04\n",
      "82 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.9, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.68; std - 0.02\n",
      "83 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.9, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.69; std - 0.03\n",
      "84 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.9, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.69; std - 0.02\n",
      "85 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.9, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.69; std - 0.05\n",
      "86 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.9, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.69; std - 0.02\n",
      "87 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.9, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.69; std - 0.03\n",
      "88 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.95, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.69; std - 0.03\n",
      "89 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.95, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.68; std - 0.04\n",
      "90 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.95, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.69; std - 0.03\n",
      "91 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.95, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.68; std - 0.03\n",
      "92 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.95, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.69; std - 0.02\n",
      "93 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.95, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.69; std - 0.04\n",
      "94 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.95, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.69; std - 0.03\n",
      "95 params - {'clf__n_neighbors': 15, 'vect__max_df': 0.95, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.69; std - 0.03\n",
      "96 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.7, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.70; std - 0.03\n",
      "97 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.7, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.72; std - 0.02\n",
      "98 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.7, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.71; std - 0.02\n",
      "99 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.7, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.72; std - 0.02\n",
      "100 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.7, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.70; std - 0.02\n",
      "101 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.7, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.71; std - 0.03\n",
      "102 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.7, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.70; std - 0.03\n",
      "103 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.7, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.72; std - 0.03\n",
      "104 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.8, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.70; std - 0.02\n",
      "105 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.8, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.73; std - 0.02\n",
      "106 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.8, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.70; std - 0.02\n",
      "107 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.8, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.73; std - 0.03\n",
      "108 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.8, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.70; std - 0.03\n",
      "109 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.8, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.72; std - 0.03\n",
      "110 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.8, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.70; std - 0.03\n",
      "111 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.8, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.71; std - 0.03\n",
      "112 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.9, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.70; std - 0.02\n",
      "113 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.9, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.71; std - 0.02\n",
      "114 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.9, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.71; std - 0.03\n",
      "115 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.9, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.72; std - 0.03\n",
      "116 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.9, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.71; std - 0.02\n",
      "117 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.9, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.70; std - 0.03\n",
      "118 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.9, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.70; std - 0.03\n",
      "119 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.9, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.70; std - 0.03\n",
      "120 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.95, 'vect__min_df': 3, 'vect__ngram_range': (1, 1)}; mean - 0.71; std - 0.02\n",
      "121 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.95, 'vect__min_df': 3, 'vect__ngram_range': (1, 3)}; mean - 0.71; std - 0.02\n",
      "122 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.95, 'vect__min_df': 4, 'vect__ngram_range': (1, 1)}; mean - 0.71; std - 0.02\n",
      "123 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.95, 'vect__min_df': 4, 'vect__ngram_range': (1, 3)}; mean - 0.72; std - 0.02\n",
      "124 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.95, 'vect__min_df': 5, 'vect__ngram_range': (1, 1)}; mean - 0.71; std - 0.03\n",
      "125 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.95, 'vect__min_df': 5, 'vect__ngram_range': (1, 3)}; mean - 0.72; std - 0.03\n",
      "126 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.95, 'vect__min_df': 6, 'vect__ngram_range': (1, 1)}; mean - 0.71; std - 0.02\n",
      "127 params - {'clf__n_neighbors': 20, 'vect__max_df': 0.95, 'vect__min_df': 6, 'vect__ngram_range': (1, 3)}; mean - 0.71; std - 0.02\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.70      0.72      0.71       239\n",
      "         pos       0.74      0.72      0.73       261\n",
      "\n",
      "    accuracy                           0.72       500\n",
      "   macro avg       0.72      0.72      0.72       500\n",
      "weighted avg       0.72      0.72      0.72       500\n",
      "\n",
      "[[172  67]\n",
      " [ 73 188]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Willi\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Performance\n",
    "\n",
    "def calculate_performance_stats(grid_search):\n",
    "    # TASK: print the mean and std for each candidate along with the parameter\n",
    "    # settings for all the candidates explored by grid search.\n",
    "    n_candidates = len(grid_search.cv_results_['params'])\n",
    "    for i in range(n_candidates):\n",
    "        print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
    "              % (grid_search.cv_results_['params'][i],\n",
    "                 grid_search.cv_results_['mean_test_score'][i],\n",
    "                 grid_search.cv_results_['std_test_score'][i]))\n",
    "\n",
    "    # TASK: Predict the outcome on the testing set and store it in a variable\n",
    "    # named y_predicted\n",
    "    y_predicted = grid_search.predict(docs_test)\n",
    "\n",
    "    # Print the classification report\n",
    "    print(metrics.classification_report(y_test, y_predicted,\n",
    "                                        target_names=dataset.target_names))\n",
    "\n",
    "    # Print and plot the confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "    print(cm)\n",
    "\n",
    "print(\"Performance of Linear: \\n\")\n",
    "calculate_performance_stats(grid_search_linear)\n",
    "\n",
    "print(\"Performance of KNN: \\n\")\n",
    "calculate_performance_stats(grid_search_knn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 (20 points): Open Ended Question:  Finding the Right Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can you find a two-dimensional plot in which the positive and negative reviews are separated?\n",
    "    * This problem is hard since you will likely have thousands of features for review, and you will need to transform these thousands of features into just two numbers (so that you can make a 2D plot).\n",
    "* Note, I was not able to find such a plot myself!\n",
    "    * So, this problem is about **trying** but perhaps **not necessarily succeeding**!\n",
    "* I tried two things, neither of which worked very well.\n",
    "    * I first plotted the length of the review versus the number of features we compute that are in that review\n",
    "    * Second I used Principle Component Analysis on a subset of the features.\n",
    "* Can you do better than I did!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create feature vectors\n",
    "vectorizer = TfidfVectorizer(min_df = 5,\n",
    "                             max_df = 0.8,\n",
    "                             sublinear_tf = True,\n",
    "                             use_idf = True)\n",
    "train_vectors = vectorizer.fit_transform(docs_train)\n",
    "test_vectors = vectorizer.transform(docs_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 4.529257s; Prediction time: 1.185212s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(train_vectors, y_train)\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "# results\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "report = classification_report(y_test, prediction_linear, output_dict=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "0.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
